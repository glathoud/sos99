# -*- coding: utf-8 -*-
#+TITLE: Soft Optimal Stop For 99% Guarantee
#+OPTIONS: author:nil date:nil toc:nil ^:nil
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="README.css" />

By [[https://glat.info][Guillaume Lathoud]], August 2025 (glat@glat.info) [[https://github.com/glathoud/sos99][Github]] [[https://glat.info/sos99/][web]] [[./README.pdf][PDF]]\\


The present document and all accompanying files are covered by the
Boost License, as described in file:./LICENSE (unless precised otherwise).\\

/Work inspired by the peculiarities of the Parisian real estate market./\\
\\

This article proposes a variation of the Secretary Problem that 
ensures high reliability of the result, i.e. maximizes the worst-case result.

#+begin_export latex
  \clearpage
#+end_export

_Starting point:_ [[https://en.wikipedia.org/wiki/Secretary_problem][Secretary Problem]]\\
The rest of the present article assumes the reader to know the
Secretary Problem. If not, please read it first. 

#+ATTR_LATEX: :height 250px
file:250px_secretary_problem.png

(illustration by cmglee - Own work, CC BY-SA 4.0, [[https://commons.wikimedia.org/w/index.php?curid=163173987][original]])\\

_Secretary Problem Strategy:_

 - ("explore") look at the first N_EXPLORE candidates
   - pick none of them

 - determine: threshold_score := best_score_of_N_EXPLORE

 - ("select") now look at the rest candidates
   - pick the first one with score > threshold_score
   - else pick the last one

In the Secretary Problem, the goal is "to have the highest
probability of selecting the best applicant of the whole group". The
best applicant is marked with a white star in the above figure.

For that goal, [[https://en.wikipedia.org/wiki/Secretary_problem][it is shown]] that the cutoff N_EXPLORE is optimal at around 37% of the
total number of candidates.\\

_Issue_

Such a goal, and its optimal stop solution (37%), sound nice ;
however 37% also means that one has a 37% chance to end up with the
fallback solution - i.e. to pick the last candidate. 

Indeed, if - like in case 3 in the above figure - one already saw
the best possible candidate (white star) before the 37% cutoff, then
one mechanically ends up picking the last candidate (marked in
yellow), which gives a pretty random result. The output can be
pretty bad, so the reliability is not guaranteed, at least not over
a single pass.

And in life, there are quite a few single pass situations.\\

_A different goal_

Let us look at a slightly different problem: guarantee with 99%
chance that we'll pick a "pretty good" candidate (not targetting the
best one).

Then we need a strategy to maximize the worst case. To that effect, we
choose to maximize the score of the 1% lowest percentile across the results of
many simulations.

I have not found existing literature examining that "reliability"
goal, see the review in Annex E. \\

_Proposed strategy:_ very similar to the Secretary Problem strategy, just a bit
softer:

 - ("explore") look at the first N_EXPLORE candidates (e.g. cutoff 37%, or any
   other percentage of the whole number of candidates)
   - pick none of them

 - determine threshold_score := soft_factor * best_score_of_N_EXPLORE
   - example soft_factor: 80%

 - ("select") now look at the rest candidates
   - pick the first one with score > threshold_score
   - else pick the last one

#+begin_export latex
  \clearpage
#+end_export

So the differences with the Secretary Problem are:

 - in the problem & evaluation: for a given value of the cutoff
   N_EXPLORE, we repeat a simulation many times and look at the score
   of the lowest 1% percentile (instead of "percentage that picked
   the best candidate").

 - in the solution: introduced a soft_factor

_One possible implementation: uniform use case_

We don't know anything about the target market, so let's assume that
the scores of the candidates are uniformly distributed, from worse
(0.0) to best (1.0). (See Annex B for an adaptation to non-uniform
score distributions).

For a relatively small total number of candidates LENGTH=100 (for
many scenarios, of a realistic order of magnitude), and various
soft_factor values, here are the corresponding implementations:

 - [[file:good_stop_other_strategy_less_demanding70pct_prctile1_simul_length100.d][soft_factor=70%]]
 - [[file:good_stop_other_strategy_less_demanding80pct_prctile1_simul_length100.d][soft_factor=80%]] (my favorite)
 - [[file:good_stop_other_strategy_less_demanding85pct_prctile1_simul_length100.d][soft_factor=85%]]
 - [[file:good_stop_other_strategy_less_demanding90pct_prctile1_simul_length100.d][soft_factor=90%]]
 - [[file:good_stop_other_strategy_less_demanding95pct_prctile1_simul_length100.d][soft_factor=95%]]

#+begin_export latex
  \clearpage
#+end_export

_Figure_: score at the lowest 1% percentile for various soft_factor values and various cutoff (N_EXPLORE) values:
 - [[file:good_stop_other_strategy_less_demanding_prctile1_simul_length100.m][octave code to produce the figure]]
 - figure (click [[file:good_stop_other_strategy_less_demanding_prctile1_simul_length100.png][here]] to open a bigger version):
file:good_stop_other_strategy_less_demanding_prctile1_simul_length100.png

In all cases, increasing too much the
cutoff N_EXPLORE leads to failure.

My favorite would be soft_factor=80% and cutoff threshold N_EXPLORE around 40/100,
which gives a score of 0.68 at the lowest 1% percentile.


When accepting the 1% risk, that result is a pretty good guarantee,
and most likely in practice we'll end up with a better pick, as shown below.

#+begin_export latex
  \clearpage
#+end_export

For soft_factor=80% and cutoff threshold around 40/100:
 - score at the  lowest 1% percentile: around 0.68
 - median score of top 99% percentile: around 0.72

Values around 0.72 can be judged as "pretty good" - our objective.\\ 

_Figure_: for soft_factor=80% and various cutoff values, score at the lowest 1% percentile, and median score of the top 99% percentiles:
 - [[file:good_stop_other_strategy_less_demanding80pct_prctile1_and_median_top99prctile__simul_length100.m][Octave code to produce the figure]]
 - figure (click [[file:good_stop_other_strategy_less_demanding80pct_prctile1_and_median_top99prctile__simul_length100.png][here]] to open a bigger version):
file:good_stop_other_strategy_less_demanding80pct_prctile1_and_median_top99prctile__simul_length100.png

#+begin_export latex
  \clearpage
#+end_export

_General Observation_

Changing the order of magnitude of LENGTH can possibly change quite
a bit the shape of the results. However, a common behaviour emerges,
similar to what the above pictures show: with increasing N_EXPLORE, the
score increases, then shows a plateau ; then when further increasing
N_EXPLORE, the score abruptly falls down to zero.

In other words, about N_EXPLORE: to get a "pretty good" result and guarantee 99% success, one
should "explore" long enough (the score increases), but not too long
either (abrupt fall to zero).\\

_Conclusion_

By *not* targetting the best candidate, but rather a "pretty good"
candidate, we built a strategy that guarantees 99% success - in fact
99.99% (Annex C below). Incremental improvements can be obtained 
with a 2-step selection approach (Annex D below). \\

_Acknowledgments_

Thanks to Julien Bourgeois for his comments. \\

_Annex A: Without scores_

As [[https://www.reddit.com/r/math/comments/1n83yht/a_variation_of_the_secretary_problem_to_guarantee/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button][pointed out by user "barely_sentient" on r/math]], in the original
Secretary Problem, the candidates are ranked in a total order, but
there is no score value attributed to them. It is still possible to
apply the strategy proposed here:

 - at the cutoff point, by sorting the candidates already seen so far, and selecting a candidate that is better than at least 80% (soft_ratio) of the other candidates seen so far. Then that candidate itself can be used as a threshold:

 - after the cutoff, to select a candidate, just compare it to that "threshold" candidate.\\

_Annex B: With scores following a non-uniform distribution_

One could in practice "uniformize" the distribution by simply
dropping the scores and applying Annex A. This should be roughly
equivalent to having a function that maps scores to another space
where the candidates are uniformly distributed.\\

#+begin_export latex
  \clearpage
#+end_export

_Annex C: Lowering the percentile from 1% down to 0.1% and 0.01%_

The approach appears to be pretty robust, in fact guaranteeing
99.99% success. See the figure below.

_Figure_: score at the lowest 1% resp. 0.1% and 0.01% percentile for various soft_factor values and various cutoff (N_EXPLORE) values:
 - [[file:good_stop_other_strategy_less_demanding80pct_prctile1_and_median_top99prctile2__simul_length100.m][octave code to produce the figure]]
 - figure (click [[file:good_stop_other_strategy_less_demanding80pct_prctile1_and_median_top99prctile2__simul_length100.png][here]] to open a bigger version):
file:good_stop_other_strategy_less_demanding80pct_prctile1_and_median_top99prctile2__simul_length100.png

#+begin_export latex
  \clearpage
#+end_export

_Annex D: Incremental improvement with a 2-step selection strategy_

Reasoning & 1st (additional) step: during the exploration phase, when we have already
explored enough, e.g. at a "pre-cutoff point" at about 66% of the
cutoff point, we can already estimate "top-tiers" to be above
(1+soft_factor)/2 of the candidates seen so far - i.e. 90% for a
soft_factor of 80%, and then, during the rest of the exploration,
select such "top-tier", if any.

2nd step: If no such "top-tier" is selected, then after the cutoff
point, we pursue the selection phase unmodified (see soft_factor
in the text above, e.g. 80% in the present example).

Such a 2-step selection strategy permits to reasonably use higher 
cutoff values while also permitting to select "top-tiers".

For a soft_factor of 80% and a cutoff threshold around 60/100

 - score at the lowest 1% percentile: around 0.75 (compare with 0.68 above)
 - median score of top 99% percentile: around 0.78 (compare with 0.72 above)

#+begin_export latex
  \clearpage
#+end_export

_Figure_: for soft_factor=80% and various cutoff values, score at the lowest 1% percentile, and median score of the top 99% percentiles:

 - [[file:good_stop_other_strategy_twostep_less_demanding80pct_prctile1_simul_length100.d][modified D code to run the simulation]]
 - [[file:good_stop_other_strategy_twostep_less_demanding80pct_prctile1_simul_length100.m][Octave code to produce the figure]]
 - figure (click [[file:good_stop_other_strategy_twostep_less_demanding80pct_prctile1_simul_length100.png][here]] to open a bigger version) - "f" is the original soft_factor result, "f2" the present 2-step selection strategy:
file:good_stop_other_strategy_twostep_less_demanding80pct_prctile1_simul_length100.png

#+begin_export latex
  \clearpage
#+end_export

Percentile analysis: lowering down from 1% to 0.1% and 0.01%
(similarly to Annex C): the 2-step strategy appears very slightly
less robust than the originally proposed one, but still very much 
guaranteeing 99.9% and 99.99% success for reasonable cutoff values
(say between around 40/100 and 60/100 for the present example).

_Figure_: score at the lowest 1% resp. 0.1% and 0.01% percentile for various soft_factor values and various cutoff (N_EXPLORE) values:
 - [[file:good_stop_other_strategy_twostep_less_demanding80pct_prctile2_simul_length100.m][octave code to produce the figure]]
 - figure (click [[file:good_stop_other_strategy_twostep_less_demanding80pct_prctile2_simul_length100.png][here]] to open a bigger version):
file:good_stop_other_strategy_twostep_less_demanding80pct_prctile2_simul_length100.png

#+begin_export latex
  \clearpage
#+end_export

_Annex E: Comparision with existing literature_

The goal of the proposed approach is "reliability": to optimize the
worst-case scenario, by maximizing the score of the 1% lowest
percentile.

In the existing literature, the two closest works I found are the following:

-- 1

https://scholarworks.indianapolis.iu.edu/server/api/core/bitstreams/ad0bc930-ec29-4915-80ac-16843de42988/content

Excerpt:

"Score-Based Secretary Problem: In the context of the Secretary Problem, suppose that the employer, after interviewing the candidates, can assign absolute scores X1, X2, . . ., which are assumed to be drawn independently from the same known continuous distribution function F . What is the employer’s best strategy to maximize the chance of winning (that is, hiring the candidate with the highest score among all n candidates)? What is the maximum probability of winning?"

=> still trying to maximize the same criterion as in the original Secretary Problem.

-- 2

https://as.nyu.edu/content/dam/nyu-as/politics/documents/faculty/brams/Secretary%20Problem.pdf

Excerpt:

"We compare three methods—Standard, Reserve (variants A and B), and Score according to three criteria:

(1) Probability of selecting the best candidate;

(2) Expected number of candidates to be interviewed; and

(3) Expected quality of the candidate selected."

=> The criterion (3) is closer to what I'm talking about, but still not maximizing the worst-case scenario - which would however probably be more relevant to a real-life scenario, i.e. an employer trying to absolutely avoid a bad match, and satisfied with a "pretty good" match. 
